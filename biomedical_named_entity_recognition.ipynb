{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkj3/vZi5kulPTwgBO2Fb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajjkavinda/biomedical-named-entity-recognition/blob/main/biomedical_named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install dependencies\n",
        "!pip install -q --upgrade transformers datasets==2.19.1 evaluate seqeval accelerate peft bioc"
      ],
      "metadata": {
        "id": "_pO2G9IUO-v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers.training_args import IntervalStrategy\n",
        "import evaluate\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from peft import PromptTuningConfig, PromptTuningInit\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "i1m_NpfePCHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load BC5CDR dataset\n",
        "dataset = load_dataset(\n",
        "    \"tner/bc5cdr\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(dataset)\n",
        "print(dataset[\"train\"].features)\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "rkUBNFYKPE4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label mapping\n",
        "label_list = [\n",
        "    \"O\",\n",
        "    \"B-Chemical\",\n",
        "    \"I-Chemical\",\n",
        "    \"B-Disease\",\n",
        "    \"I-Disease\"\n",
        "]\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(id2label)"
      ],
      "metadata": {
        "id": "Hc7PmxQ3ycSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "#Align labels\n",
        "def tokenize_and_align_labels(examples):\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples[\"tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_ds = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "HyNsUKxbPILw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrics\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "qfN1tkDHPNW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline model (Without fine-tuning)\n",
        "baseline_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "#Training configurations\n",
        "training_args_baseline = TrainingArguments(\n",
        "    output_dir=\"./baseline_results\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_baseline = Trainer(\n",
        "    model=baseline_model,\n",
        "    args=training_args_baseline,\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "baseline_results = trainer_baseline.evaluate()\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "jNVVSH6oPRX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f382e1fa"
      },
      "source": [
        "#Prompt tuning config\n",
        "prompt_config = PromptTuningConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,\n",
        "    num_virtual_tokens=20,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    prompt_tuning_init_text=\"Recognize disease entities in biomedical text.\",\n",
        "    tokenizer_name_or_path=\"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_layers=12,\n",
        "    token_dim=768,\n",
        "    num_attention_heads=12\n",
        ")\n",
        "\n",
        "#Custom data collator for prompt tuning\n",
        "class DataCollatorForPromptTuning(DataCollatorForTokenClassification):\n",
        "    def __init__(self, tokenizer, num_virtual_tokens, **kwargs):\n",
        "        super().__init__(tokenizer, **kwargs)\n",
        "        self.num_virtual_tokens = num_virtual_tokens\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = super().__call__(features)\n",
        "        virtual_token_labels = torch.full(\n",
        "            (batch[\"labels\"].shape[0], self.num_virtual_tokens),\n",
        "            -100,\n",
        "            dtype=batch[\"labels\"].dtype,\n",
        "            device=batch[\"labels\"].device\n",
        "        )\n",
        "        batch[\"labels\"] = torch.cat([virtual_token_labels, batch[\"labels\"]], dim=1)\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorForPromptTuning(tokenizer=tokenizer, num_virtual_tokens=prompt_config.num_virtual_tokens, label_pad_token_id=-100)\n",
        "\n",
        "#Apply prompt tuning\n",
        "model = get_peft_model(baseline_model, prompt_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ncbi_prompt_tuning\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "#Evaluation metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids.flatten()\n",
        "    preds = pred.predictions.argmax(-1).flatten()\n",
        "    mask = labels != -100\n",
        "    if np.sum(mask) == 0:\n",
        "        return {\n",
        "            \"precision\": 0.0,\n",
        "            \"recall\": 0.0,\n",
        "            \"f1\": 0.0,\n",
        "            \"accuracy\": 1.0\n",
        "        }\n",
        "    return {\n",
        "        \"precision\": precision_score(labels[mask], preds[mask], average=\"macro\", zero_division=0),\n",
        "        \"recall\": recall_score(labels[mask], preds[mask], average=\"macro\", zero_division=0),\n",
        "        \"f1\": f1_score(labels[mask], preds[mask], average=\"macro\", zero_division=0),\n",
        "        \"accuracy\": accuracy_score(labels[mask], preds[mask])\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "prompt_results = trainer.evaluate()\n",
        "prompt_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1127374e"
      },
      "source": [
        "#LoRA implementation\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "lora_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(lora_model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "data_collator_lora = DataCollatorForTokenClassification(tokenizer, label_pad_token_id=-100)\n",
        "\n",
        "#LoRA training configurations\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"./lora_results\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer_lora = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    data_collator=data_collator_lora,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_lora.train()\n",
        "lora_results = trainer_lora.evaluate()\n",
        "lora_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get actual keys from the results dictionary\n",
        "metrics_list = [k for k in baseline_results.keys() if k.startswith(\"eval_\") and k != \"eval_runtime\" and k != \"eval_samples_per_second\" and k != \"eval_steps_per_second\" and k != \"eval_model_preparation_time\"]\n",
        "\n",
        "#Extract scores\n",
        "baseline_scores = [baseline_results[m] for m in metrics_list]\n",
        "prompt_scores   = [prompt_results[m] for m in metrics_list]\n",
        "lora_scores     = [lora_results[m] for m in metrics_list]\n",
        "\n",
        "#Clean metric names\n",
        "metric_labels = [m.replace(\"eval_\", \"\").capitalize() for m in metrics_list]\n",
        "\n",
        "x = np.arange(len(metric_labels))\n",
        "width = 0.25\n",
        "\n",
        "#Display comparision\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x - width, baseline_scores, width, label=\"Baseline\")\n",
        "plt.bar(x, prompt_scores, width, label=\"Prompt Tuning\")\n",
        "plt.bar(x + width, lora_scores, width, label=\"LoRA PEFT\")\n",
        "plt.xticks(x, metric_labels)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0,1.05)\n",
        "plt.title(\"tner/bc5cdr NER: Baseline vs Prompt Tuning vs LoRA PEFT\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T6pLAlqlPfgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}